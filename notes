configs file:
    global variables: 
                    - sampler = optuna.samplers.TPESampler()??
                    - study = optuna.create_study(sampler=sampler, direction='minimize')
                    - r2_target = 0.99
                    - nmae_target = 0.08
                    - k : for kfold crossvalidation
                    - loss_function = F.mse_loss

                    - learning_rate = trial.suggest_loguniform("learning_rate", 0.0001, 0.01)
                    - batch_size = trial.suggest_int("batch_size", 50, 150)
                    - epochs = trial.suggest_int("epochs", 200, 200)
                    - optimizer_name = trial.suggest_categorical("opt", ["Adam"])  # , "RMSprop", "SGD"  
                    - mre_threshold ??              
                    - input variables
                    - output variables
                    
    input data path
    output data path



errors checks:

requirements.txt



â¸»

ğŸ§¹ 1. Code Cleaning
	â€¢	Remove unused code: Delete commented-out or obsolete functions.
	â€¢	Check naming consistency: Use clear, descriptive names (e.g., process_data() instead of pd()).
	â€¢	Apply formatting:
	â€¢	Use an auto-formatter like Black (Python), Prettier (JavaScript), etc.
	â€¢	Enforce linting (e.g., flake8, eslint, pylint).
	â€¢	Simplify logic: Refactor overly complex functions into smaller, focused ones.
	â€¢	Handle secrets: Remove API keys, passwords, and tokens. Add them to a .env file and a .gitignore.

â¸»

ğŸ—‚ 2. Documentation
	â€¢	README.md (the heart of any open-source repo):
	â€¢	Project overview (what it does and why it exists)
	â€¢	Installation and setup instructions
	â€¢	Usage examples
	â€¢	Contribution guidelines
	â€¢	License
	â€¢	Code comments: Explain why something is done (not just what).
	â€¢	Docstrings (if applicable): Use a consistent style like Google-style or NumPy-style docstrings.
	â€¢	Changelog: Summarize major updates and versions (optional but helpful).

â¸»

ğŸ§ª 3. Testing
	â€¢	Write or update unit tests (use pytest, unittest, etc.).
	â€¢	Add continuous integration (GitHub Actions, GitLab CI) to run tests automatically.

â¸»

âš™ï¸ 4. Repository Structure Example

my_project/
â”‚
â”œâ”€â”€ src/                # main source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ module1.py
â”‚   â””â”€â”€ module2.py
â”‚
â”œâ”€â”€ tests/              # test scripts
â”‚   â””â”€â”€ test_module1.py
â”‚
â”œâ”€â”€ docs/               # documentation (optional)
â”‚   â””â”€â”€ index.md
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ setup.py / pyproject.toml


â¸»

ğŸªª 5. License

Choose a license that matches your intent:
	â€¢	MIT â€“ very permissive
	â€¢	Apache 2.0 â€“ permissive with patent protection
	â€¢	GPL v3 â€“ requires derivatives to stay open source

You can use choosealicense.com to pick one.

â¸»

ğŸª„ Next Step

If youâ€™d like, you can upload or paste your projectâ€™s code here (or share its structure).
I can help you:
	â€¢	Clean it up systematically
	â€¢	Write the README.md
	â€¢	Add docstrings and comments
	â€¢	Suggest an appropriate license

Would you like to start by sharing the code, the repo structure, or the README draft first?

TODO:

make the model more flexible (e.g. missing inputs, input type)
errors checks:
check if better to take out while loop or to send an error message
code improvements( pas urgent:
    only one input file
    improvement: do the CV outside the epochs and not for each epoch
    remove redundant test function
    augmenter range nombre de neurone par layer
    A Better, Simpler Approach:

Remove the create_subset line:

Python

# print("Sampling 95% as a subset for training")
# train_subset_1 = create_subset(dataset_1.full_data)
Pass the full training dataset to your optimization function:

Python

find_best_model(dataset_1, dataset_1.full_data, dataset_2) 
Delete the test() call at the end. It's redundant.

Python

# test(dataset_1, train_subset_1, 'ANN_best_20Trials.pt') 
Your code will be cleaner, and you'll let your optimization function use 100% of the training.json data to find the best model, which is more effective.)

